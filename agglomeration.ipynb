{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "# Neighborhood agglomeration notebook (script-style)\n",
    "# Implements the algorithm from spec.md, including memoized neighbor graph and R^2 edge scoring.\n",
    "\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Set, Tuple, Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "from utilities import ensure_feet_crs, compute_market_value_proxy, ols_r2\n"
   ],
   "id": "81466abbf2df7de9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters (edit to your environment)\n",
    "PARCELS_PATH = \"parcels.parquet\"\n",
    "TILES_PATH = \"neighborhood_tiles.parquet\"\n",
    "OUTPUT_PREFIX = \"intermediate_tiles_\"  # parquet written as f\"{OUTPUT_PREFIX}{iter_num}.parquet\"\n",
    "TARGET_TILE_COUNT: Optional[int] = None  # desired final number of tiles\n",
    "TARGET_FIT_PCT: Optional[float] = .69\n",
    "CRS_EPSG_FEET: Optional[int] = 2236 # SW Florida\n",
    "#CRS_EPSG_FEET: Optional[int] = 2264  #guilford\n",
    "BUFFER_FEET = 30.0  # adjacency buffer per spec\n",
    "K_NEIGHBORS = 3  # spatial lag k\n",
    "RANDOM_STATE = 42  # for any deterministic ordering if needed\n"
   ],
   "id": "61d54e9604c1e180",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "os.makedirs(\"intermediates\", exist_ok=True)",
   "id": "8b6b46a687ffe057",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Build initial tile-parcel membership using spatial join (parcels within tiles)\n",
    "\n",
    "def assign_parcels_to_tiles(\n",
    "        parcels: gpd.GeoDataFrame,\n",
    "        tiles: gpd.GeoDataFrame,\n",
    "        parcel_key: str = \"parcel_key\",\n",
    "        tile_key: str = \"tile_key\"\n",
    ") -> Dict[str, Set[int]]:\n",
    "    joined = gpd.sjoin(parcels[[parcel_key, \"geometry\"]].reset_index().rename(columns={\"index\": \"pidx\"}),\n",
    "                       tiles[[tile_key, \"geometry\"]], how=\"inner\", predicate=\"within\")\n",
    "    mapping: Dict[str, Set[int]] = {}\n",
    "    for tid, grp in joined.groupby(tile_key):\n",
    "        mapping[str(tid)] = set(grp[\"pidx\"].tolist())\n",
    "    return mapping\n"
   ],
   "id": "b8f7c07c7e26de24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Tile:\n",
    "    key: str\n",
    "    geometry: object\n",
    "    r_squared: Optional[float] = np.nan  # R^2 from the merge that created this tile\n",
    "\n",
    "\n",
    "from mp_helpers import _init_pool_buffered, _pair_overlaps_area, _init_pool_edges, _score_edge_pair_worker\n",
    "from cache_io import make_cache_key, load_cache, save_cache, cache_valid, df_to_adjacency, df_to_edge_scores\n",
    "\n",
    "\n",
    "def build_adjacency(tiles_gdf: gpd.GeoDataFrame, buffer_feet: float, n_jobs: Optional[int] = None) -> Dict[str, Set[str]]:\n",
    "    # Defensive: ensure projected CRS in feet or convert buffer to data units\n",
    "    crs = tiles_gdf.crs\n",
    "    if crs is not None:\n",
    "        unit = None\n",
    "        try:\n",
    "            unit = crs.axis_info[0].unit_name.lower()\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Approximate conversion if CRS uses meters\n",
    "        if unit and unit.startswith(\"met\"):\n",
    "            buf = buffer_feet * 0.3048\n",
    "        elif unit and (\"foot\" in unit):\n",
    "            buf = buffer_feet\n",
    "        else:\n",
    "            raise Exception(f\"Unsupported CRS unit: {unit}\")\n",
    "            # Unknown or degrees – safer to treat as meters-like fallback\n",
    "            # Consider reprojecting before calling this, or pass CRS_EPSG_FEET\n",
    "            buf = buffer_feet * 0.3048\n",
    "    else:\n",
    "        # No CRS – assume feet distance intent but we can’t be sure. Treat as feet.\n",
    "        buf = buffer_feet\n",
    "\n",
    "    keys = tiles_gdf[\"tile_key\"].astype(str).tolist()\n",
    "    geoms = tiles_gdf.geometry\n",
    "    buffered = geoms.buffer(buf)\n",
    "    buffered_list: List[object] = list(buffered.values)\n",
    "\n",
    "    adjacency: Dict[str, Set[str]] = {k: set() for k in keys}\n",
    "\n",
    "    # Spatial index and candidate generation (with fallback for older GeoPandas/rtree backends)\n",
    "    buffered_gs = gpd.GeoSeries(buffered_list)\n",
    "    sindex = buffered_gs.sindex\n",
    "    cand_pairs: List[Tuple[int, int]] = []\n",
    "    print(\"Generating candidate pairs\")\n",
    "\n",
    "    # Determine jobs early so the fallback can use threads\n",
    "    if n_jobs is None:\n",
    "        # Use all but one core by default, minimum 1\n",
    "        try:\n",
    "            cpu = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            cpu = 1\n",
    "        n_jobs = max(1, cpu - 1)\n",
    "    n_jobs = max(1, int(n_jobs))\n",
    "\n",
    "    try:\n",
    "        # Preferred fast path\n",
    "        if hasattr(sindex, \"query_bulk\"):\n",
    "            left_idx, right_idx = sindex.query_bulk(buffered_gs, predicate=\"intersects\")\n",
    "            cand_pairs = [(int(i), int(j)) for i, j in zip(left_idx, right_idx) if int(j) > int(i)]\n",
    "        else:\n",
    "            raise AttributeError(\"query_bulk not available\")\n",
    "    except Exception:\n",
    "        # Fallback path: per-geometry queries using predicate when available, else bbox intersection\n",
    "        def _cand_for_i(i: int) -> List[Tuple[int, int]]:\n",
    "            geom = buffered_list[i]\n",
    "            try:\n",
    "                cand_idx = list(sindex.query(geom, predicate=\"intersects\"))\n",
    "            except Exception:\n",
    "                try:\n",
    "                    cand_idx = list(sindex.intersection(getattr(geom, \"bounds\", ())))\n",
    "                except Exception:\n",
    "                    cand_idx = []\n",
    "            out: List[Tuple[int, int]] = []\n",
    "            for j in cand_idx:\n",
    "                j = int(j)\n",
    "                if j > i:\n",
    "                    out.append((i, j))\n",
    "            return out\n",
    "\n",
    "        if n_jobs > 1:\n",
    "            print(f\"Candidate generation (fallback): threading across {n_jobs} workers...\")\n",
    "            try:\n",
    "                import concurrent.futures as _fut\n",
    "                with _fut.ThreadPoolExecutor(max_workers=n_jobs) as ex:\n",
    "                    for res in ex.map(_cand_for_i, range(len(buffered_list))):\n",
    "                        if res:\n",
    "                            cand_pairs.extend(res)\n",
    "            except Exception:\n",
    "                # If threading fails for any reason, use single-threaded loop\n",
    "                for i in range(len(buffered_list)):\n",
    "                    cand_pairs.extend(_cand_for_i(i))\n",
    "        else:\n",
    "            for i in range(len(buffered_list)):\n",
    "                cand_pairs.extend(_cand_for_i(i))\n",
    "\n",
    "    print(f\"{len(cand_pairs)} candidate pairs\")\n",
    "\n",
    "    # Determine jobs\n",
    "    if n_jobs is None:\n",
    "        # Use all but one core by default, minimum 1\n",
    "        try:\n",
    "            cpu = os.cpu_count() or 1\n",
    "        except Exception:\n",
    "            cpu = 1\n",
    "        n_jobs = max(1, cpu - 1)\n",
    "    n_jobs = max(1, int(n_jobs))\n",
    "\n",
    "    if len(cand_pairs) == 0:\n",
    "        return adjacency\n",
    "\n",
    "\n",
    "    # Single-process evaluation\n",
    "    for i, j in cand_pairs:\n",
    "        ki = keys[i]; kj = keys[j]\n",
    "        adjacency[ki].add(kj)\n",
    "        adjacency[kj].add(ki)\n",
    "    return adjacency\n",
    "\n"
   ],
   "id": "4604883ee88b730e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Edge scoring with memoization\n",
    "\n",
    "@dataclass\n",
    "class EdgeScore:\n",
    "    r2: float\n",
    "    n_obs: int\n",
    "    n_sales: int\n",
    "\n",
    "\n",
    "def score_edge(tile_a: str, tile_b: str,\n",
    "               parcel_idx_by_tile: Dict[str, Set[int]],\n",
    "               parcels: gpd.GeoDataFrame) -> EdgeScore:\n",
    "    idxs = list(parcel_idx_by_tile.get(tile_a, set()) | parcel_idx_by_tile.get(tile_b, set()))\n",
    "    if len(idxs) == 0:\n",
    "        return EdgeScore(0.0, 0, 0)\n",
    "    sub = parcels.loc[list(idxs)]\n",
    "\n",
    "    # Sales count threshold (using adj_sale_price presence)\n",
    "    n_sales = sub[\"adj_sale_price\"].notna().sum() if \"adj_sale_price\" in sub.columns else 0\n",
    "    if n_sales < 3:\n",
    "        return EdgeScore(0.0, len(sub), int(n_sales))\n",
    "\n",
    "    # Predict market_value_proxy using built and land area\n",
    "    cols = [\"market_value_proxy\", \"built_area_sqft\", \"land_area_sqft\"]\n",
    "    if not all(c in sub.columns for c in cols):\n",
    "        return EdgeScore(0.0, len(sub), int(n_sales))\n",
    "    df = sub[cols].astype(float).dropna()\n",
    "    if len(df) < 3:\n",
    "        return EdgeScore(0.0, len(sub), int(n_sales))\n",
    "\n",
    "    y = df[\"market_value_proxy\"].values\n",
    "    X = df[[\"built_area_sqft\", \"land_area_sqft\"]].values\n",
    "    r2 = ols_r2(y, X)\n",
    "    return EdgeScore(r2=float(r2), n_obs=int(len(df)), n_sales=int(n_sales))\n"
   ],
   "id": "b53f2d5c4b086297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Spatial lag: 3-NN inverse-distance weighted imputation for a numeric column.\n",
    "\n",
    "def spatial_lag_impute(points_gdf: gpd.GeoDataFrame, value_col: str, k: int = 3) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Impute missing values in value_col using k-NN inverse-distance weighted average of neighbors.\n",
    "    Uses centroids for distance computations. Returns a Series of the same length with imputed values.\n",
    "    \"\"\"\n",
    "    s = points_gdf[value_col].astype(float).copy()\n",
    "    missing_mask = s.isna()\n",
    "    if not missing_mask.any():\n",
    "        return s\n",
    "\n",
    "    # Build coordinate array\n",
    "    centroids = points_gdf.geometry.centroid\n",
    "    X = np.vstack([centroids.x.values, centroids.y.values]).T\n",
    "\n",
    "    # Fit kNN on all points\n",
    "    n_neighbors = min(k, len(points_gdf))\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\"auto\").fit(X)\n",
    "    distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "    # For each missing point, compute weighted avg from neighbors that have values.\n",
    "    for i in np.where(missing_mask.values)[0]:\n",
    "        idxs = indices[i]\n",
    "        dists = distances[i]\n",
    "        # Avoid zero distance\n",
    "        dists = np.where(dists == 0, 1e-6, dists)\n",
    "        vals = s.iloc[idxs].values\n",
    "        # If some neighbors also missing, restrict to ones with values\n",
    "        mask_has = ~np.isnan(vals)\n",
    "        if not mask_has.any():\n",
    "            continue  # nothing to impute with; leave NaN\n",
    "        vals = vals[mask_has]\n",
    "        d = dists[mask_has]\n",
    "        w = 1.0 / d\n",
    "        w = w / w.sum()\n",
    "        s.iloc[i] = np.dot(w, vals)\n",
    "    return s\n"
   ],
   "id": "583bf9ab514f8c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge mechanics and iteration\n",
    "\n",
    "def dissolve_geometries(tiles_gdf: gpd.GeoDataFrame, members: List[str], tile_index_field=\"tile_key\") -> object:\n",
    "    geoms = tiles_gdf.set_index(tile_index_field).loc[members, \"geometry\"].values.tolist()\n",
    "    return unary_union(geoms)\n",
    "\n",
    "\n",
    "def run_agglomeration(parcels_path: str = PARCELS_PATH,\n",
    "                      tiles_path: str = TILES_PATH,\n",
    "                      target_tile_count: Optional[int] = TARGET_TILE_COUNT,\n",
    "                      target_fit_pct: Optional[float] = TARGET_FIT_PCT,\n",
    "                      buffer_feet: float = BUFFER_FEET,\n",
    "                      k_neighbors: int = K_NEIGHBORS,\n",
    "                      crs_epsg_feet: Optional[int] = CRS_EPSG_FEET,\n",
    "                      output_prefix: str = OUTPUT_PREFIX,\n",
    "                      n_jobs: Optional[int] = None,\n",
    "                      cache_dir: str = \".cache\",\n",
    "                      cache_prefix: str = \"init\",\n",
    "                      use_cache: bool = True,\n",
    "                      force_rebuild: bool = False):\n",
    "    # Load data\n",
    "    parcels = gpd.read_parquet(parcels_path)\n",
    "    tiles = gpd.read_parquet(tiles_path)\n",
    "\n",
    "    # Reproject if desired\n",
    "    parcels = ensure_feet_crs(parcels, crs_epsg_feet)\n",
    "    tiles = ensure_feet_crs(tiles, crs_epsg_feet)\n",
    "\n",
    "    print(list(parcels.columns))\n",
    "\n",
    "    parcels[\"built_area_sqft\"] = parcels[\"bldg_area_finished_sqft\"] #TODO\n",
    "    parcels[\"assessed_value\"] = parcels[\"assr_market_value\"] #TODO\n",
    "    parcels[\"adj_sale_price\"] = parcels[\"sale_price_time_adj\"] #TODO\n",
    "    parcels[\"parcel_key\"] = parcels[\"key\"] #TODO\n",
    "    # Compute/impute built_area_sqft\n",
    "    if \"built_area_sqft\" not in parcels.columns:\n",
    "        print(list(parcels.columns))\n",
    "        raise ValueError(\"parcels missing built_area_sqft column\")\n",
    "    parcels[\"built_area_sqft\"] = parcels[\"built_area_sqft\"].astype(float)\n",
    "    parcels[\"built_area_sqft\"] =  parcels[\"built_area_sqft\"].fillna(0.0)\n",
    "    parcels[\"built_area_sqft\"] = spatial_lag_impute(parcels, \"built_area_sqft\", k=k_neighbors)\n",
    "\n",
    "    # Market value proxy, then spatial lag\n",
    "    parcels[\"market_value_proxy\"] = compute_market_value_proxy(parcels)\n",
    "    parcels[\"market_value_proxy\"] = spatial_lag_impute(parcels, \"market_value_proxy\", k=k_neighbors)\n",
    "\n",
    "    # Ensure land_area_sqft numeric\n",
    "    if \"land_area_sqft\" in parcels.columns:\n",
    "        parcels[\"land_area_sqft\"] = parcels[\"land_area_sqft\"].astype(float)\n",
    "\n",
    "    # Initialize tiles structure\n",
    "    tiles = tiles.copy()\n",
    "    tiles[\"tile_key\"] = tiles[\"tile_key\"].astype(str)\n",
    "    if \"r_squared\" not in tiles.columns:\n",
    "        tiles[\"r_squared\"] = np.nan\n",
    "\n",
    "    # Prefilter tiles: drop any tile that does not intersect at least one parcel (speeds adjacency)\n",
    "    try:\n",
    "        j = gpd.sjoin(tiles[[\"tile_key\", \"geometry\"]], parcels[[\"geometry\"]], how=\"inner\", predicate=\"intersects\")\n",
    "        keep_keys = set(j[\"tile_key\"].astype(str).unique().tolist())\n",
    "        before_n = len(tiles)\n",
    "        tiles = tiles[tiles[\"tile_key\"].isin(keep_keys)].reset_index(drop=True)\n",
    "        after_n = len(tiles)\n",
    "        print(f\"Prefiltered tiles by parcel intersection: {before_n} -> {after_n}\")\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Prefilter step failed; proceeding without drop. Error: {e}\")\n",
    "\n",
    "    # Initial parcel membership (on filtered tiles)\n",
    "    parcel_idx_by_tile = assign_parcels_to_tiles(parcels, tiles)\n",
    "\n",
    "    # Cache key after prefilter\n",
    "    meta_now = make_cache_key(parcels_path, tiles_path, buffer_feet, crs_epsg_feet, k_neighbors, tiles)\n",
    "\n",
    "    adjacency: Optional[Dict[str, Set[str]]] = None\n",
    "    edge_scores: Dict[frozenset, EdgeScore] = {}\n",
    "\n",
    "    if use_cache and not force_rebuild:\n",
    "        cached_meta, df_adj, df_scores = load_cache(cache_dir=cache_dir, prefix=cache_prefix)\n",
    "        if cached_meta is not None and df_adj is not None and cache_valid(cached_meta, meta_now):\n",
    "            print(\"Loading adjacency/edge scores from cache...\")\n",
    "            adjacency = df_to_adjacency(df_adj)\n",
    "            # Edge scores may be empty or missing; it's okay\n",
    "            raw_scores = df_to_edge_scores(df_scores)\n",
    "            for k, d in raw_scores.items():\n",
    "                edge_scores[k] = EdgeScore(r2=float(d.get(\"r2\", 0.0)), n_obs=int(d.get(\"n_obs\", 0)), n_sales=int(d.get(\"n_sales\", 0)))\n",
    "        else:\n",
    "            if cached_meta is not None:\n",
    "                print(\"Cache present but invalid/stale; rebuilding...\")\n",
    "\n",
    "    if adjacency is None:\n",
    "        # Build initial adjacency\n",
    "        adjacency = build_adjacency(tiles[[\"tile_key\", \"geometry\"]], buffer_feet, n_jobs=n_jobs)\n",
    "\n",
    "        # Build unique edge list from adjacency\n",
    "        _edge_pairs: List[Tuple[str, str]] = []\n",
    "        for a, nbs in adjacency.items():\n",
    "            for b in nbs:\n",
    "                if a < b:\n",
    "                    _edge_pairs.append((a, b))\n",
    "\n",
    "        print(f\"Scoring {len(_edge_pairs)} initial edges...\")\n",
    "\n",
    "        # Determine n_jobs if not provided\n",
    "        _jobs = n_jobs\n",
    "        if _jobs is None:\n",
    "            try:\n",
    "                _cpu = os.cpu_count() or 1\n",
    "            except Exception:\n",
    "                _cpu = 1\n",
    "            _jobs = max(1, _cpu - 1)\n",
    "        _jobs = max(1, int(_jobs))\n",
    "\n",
    "        if len(_edge_pairs) == 0:\n",
    "            pass\n",
    "        elif _jobs == 1:\n",
    "            # Single-thread scoring\n",
    "            for a, b in _edge_pairs:\n",
    "                edge_scores[frozenset([a, b])] = score_edge(a, b, parcel_idx_by_tile, parcels)\n",
    "        else:\n",
    "            # Parallel scoring via multiprocessing (spawn)\n",
    "            try:\n",
    "                cols_needed = [c for c in [\"market_value_proxy\", \"built_area_sqft\", \"land_area_sqft\", \"adj_sale_price\"] if c in parcels.columns]\n",
    "                parcels_min_df = parcels[cols_needed].copy()\n",
    "                chunksize = max(1, len(_edge_pairs) // (_jobs * 16))\n",
    "                print(f\"Initial edge scoring: {len(_edge_pairs):,} pairs across {_jobs} processes...\")\n",
    "                with mp.get_context(\"spawn\").Pool(processes=_jobs, initializer=_init_pool_edges, initargs=(parcels_min_df, parcel_idx_by_tile)) as pool:\n",
    "                    for a, b, r2, n_obs, n_sales in pool.imap_unordered(_score_edge_pair_worker, _edge_pairs, chunksize=chunksize):\n",
    "                        edge_scores[frozenset([a, b])] = EdgeScore(r2=float(r2), n_obs=int(n_obs), n_sales=int(n_sales))\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Parallel edge scoring failed ({e}); falling back to single-thread.\")\n",
    "                for a, b in _edge_pairs:\n",
    "                    edge_scores[frozenset([a, b])] = score_edge(a, b, parcel_idx_by_tile, parcels)\n",
    "        print(\"Initial edge scoring complete\")\n",
    "\n",
    "        # Save cache\n",
    "        try:\n",
    "            save_cache(adjacency, edge_scores, meta_now, cache_dir=cache_dir, prefix=cache_prefix)\n",
    "            print(\"Saved adjacency and initial edge scores cache.\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to save cache: {e}\")\n",
    "\n",
    "    # Define recompute helper after adjacency/edge_scores available\n",
    "    def recompute_edges_for(tile_id: str):\n",
    "        for nb in adjacency.get(tile_id, set()):  # type: ignore[union-attr]\n",
    "            key = frozenset([tile_id, nb])\n",
    "            edge_scores[key] = score_edge(tile_id, nb, parcel_idx_by_tile, parcels)\n",
    "\n",
    "    # Helper to select best edge with tie-breakers\n",
    "    def pick_best_edge() -> Optional[Tuple[str, str, EdgeScore]]:\n",
    "        if not edge_scores:\n",
    "            return None\n",
    "        # Convert to list with union parcel count for tie-break\n",
    "        candidates = []\n",
    "        for k, es in edge_scores.items():\n",
    "            if len(k) != 2:\n",
    "                continue\n",
    "            a, b = tuple(k)\n",
    "            union_count = len(parcel_idx_by_tile.get(a, set()) | parcel_idx_by_tile.get(b, set()))\n",
    "            candidates.append((a, b, es, union_count))\n",
    "        if not candidates:\n",
    "            return None\n",
    "        # First by r2 desc, then union parcel count desc, then deterministic by sorted keys\n",
    "        candidates.sort(key=lambda t: (t[2].r2, t[3], tuple(sorted([t[0], t[1]]))), reverse=True)\n",
    "        a, b, es, _ = candidates[0]\n",
    "        return a, b, es\n",
    "\n",
    "    # Iterative merge\n",
    "    iteration = 0\n",
    "    most_recent_fit_quality = 1.0\n",
    "    target_tile_count = 1 if target_tile_count is None else target_tile_count\n",
    "    target_fit_pct = 0.0 if target_fit_pct is None else target_fit_pct\n",
    "    fit_quality: list[float] = []\n",
    "    while len(tiles) > target_tile_count and most_recent_fit_quality > target_fit_pct:\n",
    "        pick = pick_best_edge()\n",
    "        if pick is None:\n",
    "            print(\"No prospective joins remain. Stopping.\")\n",
    "            break\n",
    "        a, b, es = pick\n",
    "        if es.r2 is None or not np.isfinite(es.r2):\n",
    "            es = EdgeScore(0.0, es.n_obs, es.n_sales)\n",
    "\n",
    "        most_recent_fit_quality = es.r2\n",
    "        fit_quality.append(most_recent_fit_quality)\n",
    "        # Merge a and b into new tile id\n",
    "        new_key = f\"{a}+{b}\"\n",
    "        if iteration % 10 == 0:\n",
    "            # print(f\"Iteration {iteration}: merging {a} + {b} with R^2={es.r2:.4f} (n={es.n_obs}, sales={es.n_sales}) -> {new_key}\")\n",
    "            print(f\"Iteration {iteration}: merging regions with R^2={es.r2:.4f} (n={es.n_obs}, sales={es.n_sales})\")\n",
    "\n",
    "        # Geometry union\n",
    "        new_geom = dissolve_geometries(tiles, [a, b])\n",
    "\n",
    "        # Update tiles GeoDataFrame\n",
    "        new_row = pd.DataFrame({\"tile_key\": [new_key], \"r_squared\": [es.r2]})\n",
    "        new_gdf = gpd.GeoDataFrame(new_row, geometry=[new_geom], crs=tiles.crs)\n",
    "\n",
    "        # Remove a, b; add new\n",
    "        tiles = tiles[~tiles[\"tile_key\"].isin([a, b])]\n",
    "        tiles = pd.concat([tiles, new_gdf], ignore_index=True)\n",
    "\n",
    "        # Update parcel memberships\n",
    "        parcel_idx_by_tile[new_key] = parcel_idx_by_tile.get(a, set()) | parcel_idx_by_tile.get(b, set())\n",
    "        parcel_idx_by_tile.pop(a, None)\n",
    "        parcel_idx_by_tile.pop(b, None)\n",
    "\n",
    "        # Update adjacency\n",
    "        neighbors = (adjacency.get(a, set()) | adjacency.get(b, set())) - {a, b}\n",
    "        # Remove references to a and b\n",
    "        for nb in neighbors:\n",
    "            adjacency[nb].discard(a)\n",
    "            adjacency[nb].discard(b)\n",
    "        adjacency.pop(a, None)\n",
    "        adjacency.pop(b, None)\n",
    "        # Set new adjacency entry\n",
    "        adjacency[new_key] = set()\n",
    "        for nb in neighbors:\n",
    "            if nb == new_key:\n",
    "                continue\n",
    "            # Rook check using shared boundary on current geometries\n",
    "            # We'll conservatively assume neighbors remain neighbors; validate via geometry if needed\n",
    "            adjacency[new_key].add(nb)\n",
    "            adjacency[nb].add(new_key)\n",
    "        # Recompute edges involving new_key; remove old edges involving a or b\n",
    "        keys_to_remove = [k for k in list(edge_scores.keys()) if a in k or b in k]\n",
    "        for k in keys_to_remove:\n",
    "            edge_scores.pop(k, None)\n",
    "        recompute_edges_for(new_key)\n",
    "\n",
    "        # Write intermediate parquet snapshot\n",
    "        if iteration % 200 == 0:\n",
    "            out_path = f\"intermediates/{output_prefix}{iteration}.parquet\"\n",
    "            tiles[[\"tile_key\", \"r_squared\", \"geometry\"]].to_parquet(out_path)\n",
    "            print(f\"Wrote {out_path} with {len(tiles)} tiles\")\n",
    "\n",
    "        iteration += 1\n",
    "        if len(adjacency.get(new_key, set())) == 0 and len(tiles) > target_tile_count:\n",
    "            # If the new tile has no neighbors, check if any edges remain; otherwise stop\n",
    "            if not edge_scores:\n",
    "                print(\"No edges remain after merge; stopping.\")\n",
    "                break\n",
    "\n",
    "    print(f\"Finished with {len(tiles)} tiles with a minimum fit score of {most_recent_fit_quality}.\")\n",
    "    out_path = f\"output_tiles_final.parquet\"\n",
    "    tiles[[\"tile_key\", \"r_squared\", \"geometry\"]].to_parquet(out_path)\n",
    "    print(f\"Wrote {out_path} with {len(tiles)} tiles\")\n",
    "    print(\"Saving fit quality to fit_quality.js\")\n",
    "    with open(\"fit_quality.js\", \"w\") as f:\n",
    "        json.dump(fit_quality, f)\n",
    "    return tiles\n"
   ],
   "id": "270cc21e4ba53c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example run (will execute when run as a script or in this notebook)\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        run_agglomeration()\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Skipping run: {e}. Ensure input parquet files exist.\")\n"
   ],
   "id": "6e06e7c2d05a8a52",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
